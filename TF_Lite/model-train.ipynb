{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Uo42p7BV4PV"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "35BJmtVpAP_n"
      },
      "outputs": [],
      "source": [
        "!pip install -q tflite-model-maker\n",
        "!pip install -q tflite-support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prQ86DdtD317"
      },
      "source": [
        "Import the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l4QQTXHHATDS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tflite_model_maker.config import ExportFormat, QuantizationConfig\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import object_detector\n",
        "\n",
        "from tflite_support import metadata\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g6aQvXsD78P"
      },
      "source": [
        "### Prepare the dataset\n",
        "\n",
        "This dataset contains about 50 images of 1 class -> bird\n",
        "\n",
        "We start with downloading the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AGg7D4JAV62",
        "outputId": "70ccf442-df8a-40fc-d335-0c55e272222c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-06-07 04:45:34--  https://transfer.sh/vg4Vt5/birds.zip\n",
            "Resolving transfer.sh (transfer.sh)... 144.76.136.153, 2a01:4f8:200:1097::2\n",
            "Connecting to transfer.sh (transfer.sh)|144.76.136.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1960426 (1.9M) [application/zip]\n",
            "Saving to: ‘birds.zip’\n",
            "\n",
            "birds.zip           100%[===================>]   1.87M  1.97MB/s    in 0.9s    \n",
            "\n",
            "2022-06-07 04:45:36 (1.97 MB/s) - ‘birds.zip’ saved [1960426/1960426]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://transfer.sh/vg4Vt5/birds.zip\n",
        "\n",
        "!unzip -q birds.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxh3KInCFeB-"
      },
      "source": [
        "## Train the object detection model\n",
        "\n",
        "### Step 1: Load the dataset\n",
        "\n",
        "* Images in `train_data` is used to train the custom object detection model.\n",
        "* Images in `val_data` is used to check if the model can generalize well to new images that it hasn't seen before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WiAahdsQAdT7"
      },
      "outputs": [],
      "source": [
        "train_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    'birds/train',\n",
        "    'birds/train',\n",
        "    ['bird']\n",
        ")\n",
        "\n",
        "val_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    'birds/validate',\n",
        "    'birds/validate',\n",
        "    ['bird']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNRhB8N7GHXj"
      },
      "source": [
        "### Step 2: Select a model architecture\n",
        "\n",
        "EfficientDet-Lite[0-4] are a family of mobile/IoT-friendly object detection models derived from the [EfficientDet](https://arxiv.org/abs/1911.09070) architecture.\n",
        "\n",
        "Here is the performance of each EfficientDet-Lite models compared to each others.\n",
        "\n",
        "| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n",
        "|--------------------|-----------|---------------|----------------------|\n",
        "| EfficientDet-Lite0 | 4.4       | 146           | 25.69%               |\n",
        "| EfficientDet-Lite1 | 5.8       | 259           | 30.55%               |\n",
        "| EfficientDet-Lite2 | 7.2       | 396           | 33.97%               |\n",
        "| EfficientDet-Lite3 | 11.4      | 716           | 37.70%               |\n",
        "| EfficientDet-Lite4 | 19.9      | 1886          | 41.96%               |\n",
        "\n",
        "<i> * Size of the integer quantized models. <br/>\n",
        "** Latency measured on Raspberry Pi 4 using 4 threads on CPU. <br/>\n",
        "*** Average Precision is the mAP (mean Average Precision) on the COCO 2017 validation dataset.\n",
        "</i>\n",
        "\n",
        "In this notebook, we use EfficientDet-Lite0 to train our model. You can choose other model architectures depending on whether speed or accuracy is more important to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GZOojrDHAY1J"
      },
      "outputs": [],
      "source": [
        "spec = model_spec.get('efficientdet_lite4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aeDU4mIM4ft"
      },
      "source": [
        "### Step 3: Train the TensorFlow model with the training data.\n",
        "\n",
        "* Set `epochs = 20`, which means it will go through the training dataset 20 times. You can look at the validation accuracy during training and stop when you see validation loss (`val_loss`) stop decreasing to avoid overfitting.\n",
        "* Set `batch_size = 4` here so you will see that it takes 15 steps to go through the 62 images in the training dataset.\n",
        "* Set `train_whole_model=True` to fine-tune the whole model instead of just training the head layer to improve accuracy. The trade-off is that it may take longer to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MClfpsJAfda",
        "outputId": "0050da01-7972-472b-c3b0-6e5c03ce5cca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "10/10 [==============================] - 71s 2s/step - det_loss: 1.7515 - cls_loss: 1.1401 - box_loss: 0.0122 - reg_l2_loss: 0.1078 - loss: 1.8592 - learning_rate: 0.0065 - gradient_norm: 1.6105 - val_det_loss: 1.6601 - val_cls_loss: 1.0857 - val_box_loss: 0.0115 - val_reg_l2_loss: 0.1077 - val_loss: 1.7678\n",
            "Epoch 2/30\n",
            "10/10 [==============================] - 10s 1s/step - det_loss: 1.6344 - cls_loss: 1.0725 - box_loss: 0.0112 - reg_l2_loss: 0.1077 - loss: 1.7422 - learning_rate: 0.0050 - gradient_norm: 2.0404 - val_det_loss: 1.4840 - val_cls_loss: 0.9590 - val_box_loss: 0.0105 - val_reg_l2_loss: 0.1077 - val_loss: 1.5918\n",
            "Epoch 3/30\n",
            "10/10 [==============================] - 10s 1s/step - det_loss: 1.3882 - cls_loss: 0.8845 - box_loss: 0.0101 - reg_l2_loss: 0.1078 - loss: 1.4959 - learning_rate: 0.0049 - gradient_norm: 5.2287 - val_det_loss: 1.1291 - val_cls_loss: 0.6590 - val_box_loss: 0.0094 - val_reg_l2_loss: 0.1078 - val_loss: 1.2369\n",
            "Epoch 4/30\n",
            "10/10 [==============================] - 11s 1s/step - det_loss: 1.0932 - cls_loss: 0.6360 - box_loss: 0.0091 - reg_l2_loss: 0.1078 - loss: 1.2010 - learning_rate: 0.0048 - gradient_norm: 5.4857 - val_det_loss: 1.1926 - val_cls_loss: 0.7570 - val_box_loss: 0.0087 - val_reg_l2_loss: 0.1078 - val_loss: 1.3003\n",
            "Epoch 5/30\n",
            "10/10 [==============================] - 17s 2s/step - det_loss: 0.9330 - cls_loss: 0.4985 - box_loss: 0.0087 - reg_l2_loss: 0.1078 - loss: 1.0408 - learning_rate: 0.0047 - gradient_norm: 4.5533 - val_det_loss: 0.7727 - val_cls_loss: 0.4001 - val_box_loss: 0.0075 - val_reg_l2_loss: 0.1078 - val_loss: 0.8805\n",
            "Epoch 6/30\n",
            "10/10 [==============================] - 10s 1s/step - det_loss: 0.7612 - cls_loss: 0.4270 - box_loss: 0.0067 - reg_l2_loss: 0.1078 - loss: 0.8690 - learning_rate: 0.0046 - gradient_norm: 4.9216 - val_det_loss: 0.6926 - val_cls_loss: 0.3616 - val_box_loss: 0.0066 - val_reg_l2_loss: 0.1078 - val_loss: 0.8004\n",
            "Epoch 7/30\n",
            "10/10 [==============================] - 11s 1s/step - det_loss: 0.7568 - cls_loss: 0.3935 - box_loss: 0.0073 - reg_l2_loss: 0.1078 - loss: 0.8646 - learning_rate: 0.0044 - gradient_norm: 4.1663 - val_det_loss: 0.6718 - val_cls_loss: 0.3475 - val_box_loss: 0.0065 - val_reg_l2_loss: 0.1078 - val_loss: 0.7796\n",
            "Epoch 8/30\n",
            "10/10 [==============================] - 10s 1s/step - det_loss: 0.6724 - cls_loss: 0.3511 - box_loss: 0.0064 - reg_l2_loss: 0.1078 - loss: 0.7802 - learning_rate: 0.0042 - gradient_norm: 4.0873 - val_det_loss: 0.6217 - val_cls_loss: 0.3339 - val_box_loss: 0.0058 - val_reg_l2_loss: 0.1078 - val_loss: 0.7296\n",
            "Epoch 9/30\n",
            "10/10 [==============================] - 10s 1s/step - det_loss: 0.6183 - cls_loss: 0.3429 - box_loss: 0.0055 - reg_l2_loss: 0.1078 - loss: 0.7261 - learning_rate: 0.0040 - gradient_norm: 4.7049 - val_det_loss: 0.5966 - val_cls_loss: 0.3198 - val_box_loss: 0.0055 - val_reg_l2_loss: 0.1078 - val_loss: 0.7044\n",
            "Epoch 10/30\n",
            "10/10 [==============================] - 12s 1s/step - det_loss: 0.5993 - cls_loss: 0.3212 - box_loss: 0.0056 - reg_l2_loss: 0.1078 - loss: 0.7071 - learning_rate: 0.0038 - gradient_norm: 4.5937 - val_det_loss: 0.5806 - val_cls_loss: 0.2926 - val_box_loss: 0.0058 - val_reg_l2_loss: 0.1078 - val_loss: 0.6884\n",
            "Epoch 11/30\n",
            "10/10 [==============================] - 10s 1s/step - det_loss: 0.5978 - cls_loss: 0.3289 - box_loss: 0.0054 - reg_l2_loss: 0.1078 - loss: 0.7056 - learning_rate: 0.0036 - gradient_norm: 5.2940 - val_det_loss: 0.5625 - val_cls_loss: 0.2702 - val_box_loss: 0.0058 - val_reg_l2_loss: 0.1078 - val_loss: 0.6704\n",
            "Epoch 12/30\n",
            "10/10 [==============================] - 10s 1s/step - det_loss: 0.5728 - cls_loss: 0.3140 - box_loss: 0.0052 - reg_l2_loss: 0.1078 - loss: 0.6807 - learning_rate: 0.0033 - gradient_norm: 4.7607 - val_det_loss: 0.4793 - val_cls_loss: 0.2518 - val_box_loss: 0.0045 - val_reg_l2_loss: 0.1079 - val_loss: 0.5871\n",
            "Epoch 13/30\n",
            "10/10 [==============================] - 11s 1s/step - det_loss: 0.5328 - cls_loss: 0.3030 - box_loss: 0.0046 - reg_l2_loss: 0.1079 - loss: 0.6406 - learning_rate: 0.0030 - gradient_norm: 3.9500 - val_det_loss: 0.4979 - val_cls_loss: 0.2554 - val_box_loss: 0.0049 - val_reg_l2_loss: 0.1079 - val_loss: 0.6058\n",
            "Epoch 14/30\n",
            "10/10 [==============================] - 10s 1s/step - det_loss: 0.4893 - cls_loss: 0.2773 - box_loss: 0.0042 - reg_l2_loss: 0.1079 - loss: 0.5971 - learning_rate: 0.0028 - gradient_norm: 3.5037 - val_det_loss: 0.5004 - val_cls_loss: 0.2490 - val_box_loss: 0.0050 - val_reg_l2_loss: 0.1079 - val_loss: 0.6082\n",
            "Epoch 15/30\n",
            "10/10 [==============================] - 11s 1s/step - det_loss: 0.5135 - cls_loss: 0.3054 - box_loss: 0.0042 - reg_l2_loss: 0.1079 - loss: 0.6214 - learning_rate: 0.0025 - gradient_norm: 4.3785 - val_det_loss: 0.4738 - val_cls_loss: 0.2452 - val_box_loss: 0.0046 - val_reg_l2_loss: 0.1079 - val_loss: 0.5817\n",
            "Epoch 16/30\n",
            "10/10 [==============================] - 11s 1s/step - det_loss: 0.4879 - cls_loss: 0.2787 - box_loss: 0.0042 - reg_l2_loss: 0.1079 - loss: 0.5958 - learning_rate: 0.0022 - gradient_norm: 4.0301 - val_det_loss: 0.4347 - val_cls_loss: 0.2401 - val_box_loss: 0.0039 - val_reg_l2_loss: 0.1079 - val_loss: 0.5426\n",
            "Epoch 17/30\n",
            "10/10 [==============================] - 10s 1s/step - det_loss: 0.4720 - cls_loss: 0.2613 - box_loss: 0.0042 - reg_l2_loss: 0.1079 - loss: 0.5799 - learning_rate: 0.0020 - gradient_norm: 3.4004 - val_det_loss: 0.4744 - val_cls_loss: 0.2492 - val_box_loss: 0.0045 - val_reg_l2_loss: 0.1079 - val_loss: 0.5823\n",
            "Epoch 18/30\n",
            "10/10 [==============================] - 10s 1s/step - det_loss: 0.4538 - cls_loss: 0.2657 - box_loss: 0.0038 - reg_l2_loss: 0.1079 - loss: 0.5616 - learning_rate: 0.0017 - gradient_norm: 4.5565 - val_det_loss: 0.4499 - val_cls_loss: 0.2501 - val_box_loss: 0.0040 - val_reg_l2_loss: 0.1079 - val_loss: 0.5578\n",
            "Epoch 19/30\n",
            "10/10 [==============================] - 10s 1s/step - det_loss: 0.4350 - cls_loss: 0.2558 - box_loss: 0.0036 - reg_l2_loss: 0.1079 - loss: 0.5429 - learning_rate: 0.0015 - gradient_norm: 3.3104 - val_det_loss: 0.4241 - val_cls_loss: 0.2384 - val_box_loss: 0.0037 - val_reg_l2_loss: 0.1079 - val_loss: 0.5320\n",
            "Epoch 20/30\n",
            "10/10 [==============================] - 12s 1s/step - det_loss: 0.4829 - cls_loss: 0.2722 - box_loss: 0.0042 - reg_l2_loss: 0.1079 - loss: 0.5908 - learning_rate: 0.0012 - gradient_norm: 3.4274 - val_det_loss: 0.4691 - val_cls_loss: 0.2441 - val_box_loss: 0.0045 - val_reg_l2_loss: 0.1079 - val_loss: 0.5770\n",
            "Epoch 21/30\n",
            "10/10 [==============================] - 10s 1s/step - det_loss: 0.4188 - cls_loss: 0.2526 - box_loss: 0.0033 - reg_l2_loss: 0.1079 - loss: 0.5267 - learning_rate: 9.8984e-04 - gradient_norm: 4.0188 - val_det_loss: 0.4481 - val_cls_loss: 0.2432 - val_box_loss: 0.0041 - val_reg_l2_loss: 0.1079 - val_loss: 0.5559\n",
            "Epoch 22/30\n",
            "10/10 [==============================] - 10s 1s/step - det_loss: 0.4481 - cls_loss: 0.2603 - box_loss: 0.0038 - reg_l2_loss: 0.1079 - loss: 0.5560 - learning_rate: 7.8346e-04 - gradient_norm: 3.5720 - val_det_loss: 0.4020 - val_cls_loss: 0.2377 - val_box_loss: 0.0033 - val_reg_l2_loss: 0.1079 - val_loss: 0.5099\n",
            "Epoch 23/30\n",
            "10/10 [==============================] - 11s 1s/step - det_loss: 0.4342 - cls_loss: 0.2531 - box_loss: 0.0036 - reg_l2_loss: 0.1079 - loss: 0.5421 - learning_rate: 5.9721e-04 - gradient_norm: 3.2304 - val_det_loss: 0.4037 - val_cls_loss: 0.2375 - val_box_loss: 0.0033 - val_reg_l2_loss: 0.1079 - val_loss: 0.5116\n",
            "Epoch 24/30\n",
            "10/10 [==============================] - 10s 1s/step - det_loss: 0.4841 - cls_loss: 0.2839 - box_loss: 0.0040 - reg_l2_loss: 0.1079 - loss: 0.5920 - learning_rate: 4.3327e-04 - gradient_norm: 3.8872 - val_det_loss: 0.4149 - val_cls_loss: 0.2368 - val_box_loss: 0.0036 - val_reg_l2_loss: 0.1079 - val_loss: 0.5228\n",
            "Epoch 25/30\n",
            "10/10 [==============================] - 11s 1s/step - det_loss: 0.4491 - cls_loss: 0.2734 - box_loss: 0.0035 - reg_l2_loss: 0.1079 - loss: 0.5570 - learning_rate: 2.9356e-04 - gradient_norm: 4.0151 - val_det_loss: 0.4190 - val_cls_loss: 0.2364 - val_box_loss: 0.0037 - val_reg_l2_loss: 0.1079 - val_loss: 0.5269\n",
            "Epoch 26/30\n",
            "10/10 [==============================] - 11s 1s/step - det_loss: 0.4066 - cls_loss: 0.2388 - box_loss: 0.0034 - reg_l2_loss: 0.1079 - loss: 0.5145 - learning_rate: 1.7972e-04 - gradient_norm: 3.0390 - val_det_loss: 0.4125 - val_cls_loss: 0.2341 - val_box_loss: 0.0036 - val_reg_l2_loss: 0.1079 - val_loss: 0.5204\n",
            "Epoch 27/30\n",
            "10/10 [==============================] - 10s 1s/step - det_loss: 0.3900 - cls_loss: 0.2421 - box_loss: 0.0030 - reg_l2_loss: 0.1079 - loss: 0.4978 - learning_rate: 9.3080e-05 - gradient_norm: 3.3075 - val_det_loss: 0.4058 - val_cls_loss: 0.2317 - val_box_loss: 0.0035 - val_reg_l2_loss: 0.1079 - val_loss: 0.5136\n",
            "Epoch 28/30\n",
            "10/10 [==============================] - 10s 1s/step - det_loss: 0.4280 - cls_loss: 0.2592 - box_loss: 0.0034 - reg_l2_loss: 0.1079 - loss: 0.5359 - learning_rate: 3.4660e-05 - gradient_norm: 3.5577 - val_det_loss: 0.4016 - val_cls_loss: 0.2304 - val_box_loss: 0.0034 - val_reg_l2_loss: 0.1079 - val_loss: 0.5094\n",
            "Epoch 29/30\n",
            "10/10 [==============================] - 10s 1s/step - det_loss: 0.4233 - cls_loss: 0.2505 - box_loss: 0.0035 - reg_l2_loss: 0.1079 - loss: 0.5312 - learning_rate: 5.1443e-06 - gradient_norm: 3.4948 - val_det_loss: 0.4009 - val_cls_loss: 0.2306 - val_box_loss: 0.0034 - val_reg_l2_loss: 0.1079 - val_loss: 0.5088\n",
            "Epoch 30/30\n",
            "10/10 [==============================] - 12s 1s/step - det_loss: 0.4239 - cls_loss: 0.2516 - box_loss: 0.0034 - reg_l2_loss: 0.1079 - loss: 0.5318 - learning_rate: 4.8781e-06 - gradient_norm: 3.1788 - val_det_loss: 0.3985 - val_cls_loss: 0.2297 - val_box_loss: 0.0034 - val_reg_l2_loss: 0.1079 - val_loss: 0.5064\n"
          ]
        }
      ],
      "source": [
        "model = object_detector.create(train_data, model_spec=spec, batch_size=4, train_whole_model=True, epochs=30, validation_data=val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB4hKeerMmh4"
      },
      "source": [
        "### Step 4. Evaluate the model with the validation data.\n",
        "\n",
        "After training the object detection model using the images in the training dataset, use the 10 images in the validation dataset to evaluate how the model performs against new data it has never seen before.\n",
        "\n",
        "As the default batch size is 64, it will take 1 step to go through the 10 images in the validation dataset.\n",
        "\n",
        "The evaluation metrics are same as [COCO](https://cocodataset.org/#detection-eval)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUqEpcYwAg8L",
        "outputId": "31ef4d26-bc19-4cdd-fcf1-d4e688482d20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r1/1 [==============================] - 6s 6s/step\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'AP': 0.562958,\n",
              " 'AP50': 1.0,\n",
              " 'AP75': 0.62218153,\n",
              " 'AP_/bird': 0.562958,\n",
              " 'APl': 0.64865434,\n",
              " 'APm': 0.5046708,\n",
              " 'APs': -1.0,\n",
              " 'ARl': 0.71,\n",
              " 'ARm': 0.56363636,\n",
              " 'ARmax1': 0.31428573,\n",
              " 'ARmax10': 0.6333333,\n",
              " 'ARmax100': 0.6333333,\n",
              " 'ARs': -1.0}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NARVYk9rGLIl"
      },
      "source": [
        "### Step 5: Export as a TensorFlow Lite model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_u3eFxoBAiqE"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.', tflite_filename='bird_model.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZcBmEigOCO3"
      },
      "source": [
        "### Step 6:  Evaluate the TensorFlow Lite model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jbl8z9_wBPlr",
        "outputId": "320935e9-e062-4591-8c13-215532f8fb0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11/11 [==============================] - 526s 48s/step\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'AP': 0.5550389,\n",
              " 'AP50': 1.0,\n",
              " 'AP75': 0.6289486,\n",
              " 'AP_/bird': 0.5550389,\n",
              " 'APl': 0.62072134,\n",
              " 'APm': 0.50916547,\n",
              " 'APs': -1.0,\n",
              " 'ARl': 0.65,\n",
              " 'ARm': 0.55454546,\n",
              " 'ARmax1': 0.31904763,\n",
              " 'ARmax10': 0.5952381,\n",
              " 'ARmax100': 0.6,\n",
              " 'ARs': -1.0}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate_tflite('bird_model2.tflite', val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "v7zgUkdOUUnD",
        "outputId": "3a2ac443-7435-4057-9e87-fc178e5db298"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_fb922f5e-635a-4a8f-ba2e-4a385982ec2f\", \"bird_model.tflite\", 20546291)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Download the TFLite model to your local computer.\n",
        "from google.colab import files\n",
        "files.download('bird_model.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnqktl45PZRy"
      },
      "source": [
        "## Test the Android figurine detection model\n",
        "\n",
        "After training the model, let's test it with an image that the model hasn't seen before to get a sense of how good the model is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9ZsLQtJ1AlW_"
      },
      "outputs": [],
      "source": [
        "#@title Load the trained TFLite model and define some visualization functions\n",
        "\n",
        "#@markdown This code comes from the TFLite Object Detection [Raspberry Pi sample](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/raspberry_pi).\n",
        "\n",
        "import platform\n",
        "from typing import List, NamedTuple\n",
        "import json\n",
        "\n",
        "import cv2\n",
        "\n",
        "Interpreter = tf.lite.Interpreter\n",
        "load_delegate = tf.lite.experimental.load_delegate\n",
        "\n",
        "# pylint: enable=g-import-not-at-top\n",
        "\n",
        "\n",
        "class ObjectDetectorOptions(NamedTuple):\n",
        "  \"\"\"A config to initialize an object detector.\"\"\"\n",
        "\n",
        "  enable_edgetpu: bool = False\n",
        "  \"\"\"Enable the model to run on EdgeTPU.\"\"\"\n",
        "\n",
        "  label_allow_list: List[str] = None\n",
        "  \"\"\"The optional allow list of labels.\"\"\"\n",
        "\n",
        "  label_deny_list: List[str] = None\n",
        "  \"\"\"The optional deny list of labels.\"\"\"\n",
        "\n",
        "  max_results: int = -1\n",
        "  \"\"\"The maximum number of top-scored detection results to return.\"\"\"\n",
        "\n",
        "  num_threads: int = 1\n",
        "  \"\"\"The number of CPU threads to be used.\"\"\"\n",
        "\n",
        "  score_threshold: float = 0.0\n",
        "  \"\"\"The score threshold of detection results to return.\"\"\"\n",
        "\n",
        "\n",
        "class Rect(NamedTuple):\n",
        "  \"\"\"A rectangle in 2D space.\"\"\"\n",
        "  left: float\n",
        "  top: float\n",
        "  right: float\n",
        "  bottom: float\n",
        "\n",
        "\n",
        "class Category(NamedTuple):\n",
        "  \"\"\"A result of a classification task.\"\"\"\n",
        "  label: str\n",
        "  score: float\n",
        "  index: int\n",
        "\n",
        "\n",
        "class Detection(NamedTuple):\n",
        "  \"\"\"A detected object as the result of an ObjectDetector.\"\"\"\n",
        "  bounding_box: Rect\n",
        "  categories: List[Category]\n",
        "\n",
        "\n",
        "def edgetpu_lib_name():\n",
        "  \"\"\"Returns the library name of EdgeTPU in the current platform.\"\"\"\n",
        "  return {\n",
        "      'Darwin': 'libedgetpu.1.dylib',\n",
        "      'Linux': 'libedgetpu.so.1',\n",
        "      'Windows': 'edgetpu.dll',\n",
        "  }.get(platform.system(), None)\n",
        "\n",
        "\n",
        "class ObjectDetector:\n",
        "  \"\"\"A wrapper class for a TFLite object detection model.\"\"\"\n",
        "\n",
        "  _OUTPUT_LOCATION_NAME = 'location'\n",
        "  _OUTPUT_CATEGORY_NAME = 'category'\n",
        "  _OUTPUT_SCORE_NAME = 'score'\n",
        "  _OUTPUT_NUMBER_NAME = 'number of detections'\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      model_path: str,\n",
        "      options: ObjectDetectorOptions = ObjectDetectorOptions()\n",
        "  ) -> None:\n",
        "    \"\"\"Initialize a TFLite object detection model.\n",
        "    Args:\n",
        "        model_path: Path to the TFLite model.\n",
        "        options: The config to initialize an object detector. (Optional)\n",
        "    Raises:\n",
        "        ValueError: If the TFLite model is invalid.\n",
        "        OSError: If the current OS isn't supported by EdgeTPU.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load metadata from model.\n",
        "    displayer = metadata.MetadataDisplayer.with_model_file(model_path)\n",
        "\n",
        "    # Save model metadata for preprocessing later.\n",
        "    model_metadata = json.loads(displayer.get_metadata_json())\n",
        "    process_units = model_metadata['subgraph_metadata'][0]['input_tensor_metadata'][0]['process_units']\n",
        "    mean = 0.0\n",
        "    std = 1.0\n",
        "    for option in process_units:\n",
        "      if option['options_type'] == 'NormalizationOptions':\n",
        "        mean = option['options']['mean'][0]\n",
        "        std = option['options']['std'][0]\n",
        "    self._mean = mean\n",
        "    self._std = std\n",
        "\n",
        "    # Load label list from metadata.\n",
        "    file_name = displayer.get_packed_associated_file_list()[0]\n",
        "    label_map_file = displayer.get_associated_file_buffer(file_name).decode()\n",
        "    label_list = list(filter(lambda x: len(x) > 0, label_map_file.splitlines()))\n",
        "    self._label_list = label_list\n",
        "\n",
        "    # Initialize TFLite model.\n",
        "    if options.enable_edgetpu:\n",
        "      if edgetpu_lib_name() is None:\n",
        "        raise OSError(\"The current OS isn't supported by Coral EdgeTPU.\")\n",
        "      interpreter = Interpreter(\n",
        "          model_path=model_path,\n",
        "          experimental_delegates=[load_delegate(edgetpu_lib_name())],\n",
        "          num_threads=options.num_threads)\n",
        "    else:\n",
        "      interpreter = Interpreter(\n",
        "          model_path=model_path, num_threads=options.num_threads)\n",
        "\n",
        "    interpreter.allocate_tensors()\n",
        "    input_detail = interpreter.get_input_details()[0]\n",
        "\n",
        "    # From TensorFlow 2.6, the order of the outputs become undefined.\n",
        "    # Therefore we need to sort the tensor indices of TFLite outputs and to know\n",
        "    # exactly the meaning of each output tensor. For example, if\n",
        "    # output indices are [601, 599, 598, 600], tensor names and indices aligned\n",
        "    # are:\n",
        "    #   - location: 598\n",
        "    #   - category: 599\n",
        "    #   - score: 600\n",
        "    #   - detection_count: 601\n",
        "    # because of the op's ports of TFLITE_DETECTION_POST_PROCESS\n",
        "    # (https://github.com/tensorflow/tensorflow/blob/a4fe268ea084e7d323133ed7b986e0ae259a2bc7/tensorflow/lite/kernels/detection_postprocess.cc#L47-L50).\n",
        "    sorted_output_indices = sorted(\n",
        "        [output['index'] for output in interpreter.get_output_details()])\n",
        "    self._output_indices = {\n",
        "        self._OUTPUT_LOCATION_NAME: sorted_output_indices[0],\n",
        "        self._OUTPUT_CATEGORY_NAME: sorted_output_indices[1],\n",
        "        self._OUTPUT_SCORE_NAME: sorted_output_indices[2],\n",
        "        self._OUTPUT_NUMBER_NAME: sorted_output_indices[3],\n",
        "    }\n",
        "\n",
        "    self._input_size = input_detail['shape'][2], input_detail['shape'][1]\n",
        "    self._is_quantized_input = input_detail['dtype'] == np.uint8\n",
        "    self._interpreter = interpreter\n",
        "    self._options = options\n",
        "\n",
        "  def detect(self, input_image: np.ndarray) -> List[Detection]:\n",
        "    \"\"\"Run detection on an input image.\n",
        "    Args:\n",
        "        input_image: A [height, width, 3] RGB image. Note that height and width\n",
        "          can be anything since the image will be immediately resized according\n",
        "          to the needs of the model within this function.\n",
        "    Returns:\n",
        "        A Person instance.\n",
        "    \"\"\"\n",
        "    image_height, image_width, _ = input_image.shape\n",
        "\n",
        "    input_tensor = self._preprocess(input_image)\n",
        "\n",
        "    self._set_input_tensor(input_tensor)\n",
        "    self._interpreter.invoke()\n",
        "\n",
        "    # Get all output details\n",
        "    boxes = self._get_output_tensor(self._OUTPUT_LOCATION_NAME)\n",
        "    classes = self._get_output_tensor(self._OUTPUT_CATEGORY_NAME)\n",
        "    scores = self._get_output_tensor(self._OUTPUT_SCORE_NAME)\n",
        "    count = int(self._get_output_tensor(self._OUTPUT_NUMBER_NAME))\n",
        "\n",
        "    return self._postprocess(boxes, classes, scores, count, image_width,\n",
        "                             image_height)\n",
        "\n",
        "  def _preprocess(self, input_image: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Preprocess the input image as required by the TFLite model.\"\"\"\n",
        "\n",
        "    # Resize the input\n",
        "    input_tensor = cv2.resize(input_image, self._input_size)\n",
        "\n",
        "    # Normalize the input if it's a float model (aka. not quantized)\n",
        "    if not self._is_quantized_input:\n",
        "      input_tensor = (np.float32(input_tensor) - self._mean) / self._std\n",
        "\n",
        "    # Add batch dimension\n",
        "    input_tensor = np.expand_dims(input_tensor, axis=0)\n",
        "\n",
        "    return input_tensor\n",
        "\n",
        "  def _set_input_tensor(self, image):\n",
        "    \"\"\"Sets the input tensor.\"\"\"\n",
        "    tensor_index = self._interpreter.get_input_details()[0]['index']\n",
        "    input_tensor = self._interpreter.tensor(tensor_index)()[0]\n",
        "    input_tensor[:, :] = image\n",
        "\n",
        "  def _get_output_tensor(self, name):\n",
        "    \"\"\"Returns the output tensor at the given index.\"\"\"\n",
        "    output_index = self._output_indices[name]\n",
        "    tensor = np.squeeze(self._interpreter.get_tensor(output_index))\n",
        "    return tensor\n",
        "\n",
        "  def _postprocess(self, boxes: np.ndarray, classes: np.ndarray,\n",
        "                   scores: np.ndarray, count: int, image_width: int,\n",
        "                   image_height: int) -> List[Detection]:\n",
        "    \"\"\"Post-process the output of TFLite model into a list of Detection objects.\n",
        "    Args:\n",
        "        boxes: Bounding boxes of detected objects from the TFLite model.\n",
        "        classes: Class index of the detected objects from the TFLite model.\n",
        "        scores: Confidence scores of the detected objects from the TFLite model.\n",
        "        count: Number of detected objects from the TFLite model.\n",
        "        image_width: Width of the input image.\n",
        "        image_height: Height of the input image.\n",
        "    Returns:\n",
        "        A list of Detection objects detected by the TFLite model.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Parse the model output into a list of Detection entities.\n",
        "    for i in range(count):\n",
        "      if scores[i] >= self._options.score_threshold:\n",
        "        y_min, x_min, y_max, x_max = boxes[i]\n",
        "        bounding_box = Rect(\n",
        "            top=int(y_min * image_height),\n",
        "            left=int(x_min * image_width),\n",
        "            bottom=int(y_max * image_height),\n",
        "            right=int(x_max * image_width))\n",
        "        class_id = int(classes[i])\n",
        "        category = Category(\n",
        "            score=scores[i],\n",
        "            label=self._label_list[class_id],  # 0 is reserved for background\n",
        "            index=class_id)\n",
        "        result = Detection(bounding_box=bounding_box, categories=[category])\n",
        "        results.append(result)\n",
        "\n",
        "    # Sort detection results by score ascending\n",
        "    sorted_results = sorted(\n",
        "        results,\n",
        "        key=lambda detection: detection.categories[0].score,\n",
        "        reverse=True)\n",
        "\n",
        "    # Filter out detections in deny list\n",
        "    filtered_results = sorted_results\n",
        "    if self._options.label_deny_list is not None:\n",
        "      filtered_results = list(\n",
        "          filter(\n",
        "              lambda detection: detection.categories[0].label not in self.\n",
        "              _options.label_deny_list, filtered_results))\n",
        "\n",
        "    # Keep only detections in allow list\n",
        "    if self._options.label_allow_list is not None:\n",
        "      filtered_results = list(\n",
        "          filter(\n",
        "              lambda detection: detection.categories[0].label in self._options.\n",
        "              label_allow_list, filtered_results))\n",
        "\n",
        "    # Only return maximum of max_results detection.\n",
        "    if self._options.max_results > 0:\n",
        "      result_count = min(len(filtered_results), self._options.max_results)\n",
        "      filtered_results = filtered_results[:result_count]\n",
        "\n",
        "    return filtered_results\n",
        "\n",
        "\n",
        "_MARGIN = 10  # pixels\n",
        "_ROW_SIZE = 10  # pixels\n",
        "_FONT_SIZE = 1\n",
        "_FONT_THICKNESS = 1\n",
        "_TEXT_COLOR = (0, 0, 255)  # red\n",
        "\n",
        "\n",
        "def visualize(\n",
        "    image: np.ndarray,\n",
        "    detections: List[Detection],\n",
        ") -> np.ndarray:\n",
        "  \"\"\"Draws bounding boxes on the input image and return it.\n",
        "  Args:\n",
        "    image: The input RGB image.\n",
        "    detections: The list of all \"Detection\" entities to be visualize.\n",
        "  Returns:\n",
        "    Image with bounding boxes.\n",
        "  \"\"\"\n",
        "  for detection in detections:\n",
        "    # Draw bounding_box\n",
        "    start_point = detection.bounding_box.left, detection.bounding_box.top\n",
        "    end_point = detection.bounding_box.right, detection.bounding_box.bottom\n",
        "    cv2.rectangle(image, start_point, end_point, _TEXT_COLOR, 3)\n",
        "\n",
        "    # Draw label and score\n",
        "    category = detection.categories[0]\n",
        "    class_name = category.label\n",
        "    probability = round(category.score, 2)\n",
        "    result_text = class_name + ' (' + str(probability) + ')'\n",
        "    text_location = (_MARGIN + detection.bounding_box.left,\n",
        "                     _MARGIN + _ROW_SIZE + detection.bounding_box.top)\n",
        "    cv2.putText(image, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
        "                _FONT_SIZE, _TEXT_COLOR, _FONT_THICKNESS)\n",
        "\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1t1z2fKlAoB0"
      },
      "outputs": [],
      "source": [
        "#@title Run object detection and show the detection results\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "INPUT_IMAGE_URL = \"http://download.tensorflow.org/example_images/android_figurine.jpg\" #@param {type:\"string\"}\n",
        "DETECTION_THRESHOLD = 0.5 #@param {type:\"number\"}\n",
        "TFLITE_MODEL_PATH = \"android.tflite\" #@param {type:\"string\"}\n",
        "\n",
        "TEMP_FILE = '/tmp/image.png'\n",
        "\n",
        "!wget -q -O $TEMP_FILE $INPUT_IMAGE_URL\n",
        "image = Image.open(TEMP_FILE).convert('RGB')\n",
        "image.thumbnail((512, 512), Image.ANTIALIAS)\n",
        "image_np = np.asarray(image)\n",
        "\n",
        "# Load the TFLite model\n",
        "options = ObjectDetectorOptions(\n",
        "      num_threads=4,\n",
        "      score_threshold=DETECTION_THRESHOLD,\n",
        ")\n",
        "detector = ObjectDetector(model_path=TFLITE_MODEL_PATH, options=options)\n",
        "\n",
        "# Run object detection estimation using the model.\n",
        "detections = detector.detect(image_np)\n",
        "\n",
        "# Draw keypoints and edges on input image\n",
        "image_np = visualize(image_np, detections)\n",
        "\n",
        "# Show the detection result\n",
        "Image.fromarray(image_np)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "VISIOT - TF Lite - Bird",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
